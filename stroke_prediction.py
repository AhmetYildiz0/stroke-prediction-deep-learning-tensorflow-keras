# -*- coding: utf-8 -*-
"""Stroke_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n-4FjfvTzjNC4fkfHSoZdTRbzv_bfuD5
"""

# This part for connect drive.
# If yours working local pc then skip this part
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/yapay_zeka_video/S-002-Stroke-Prediction')
import pandas as pd
import matplotlib.pyplot as plt
from seaborn import heatmap

from sklearn.preprocessing import LabelEncoder,MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

import tensorflow as tf
from tensorflow.keras.layers import Dropout,Dense,InputLayer

# dataset from https://www.kaggle.com/fedesoriano/stroke-prediction-dataset
# upload dataset
data=pd.read_csv('healthcare-dataset-stroke-data.csv')
# drop the id. Because we not need.
data.drop(['id'],axis=1,inplace=True)
data.head(10)

# show features of dataset
print("\n<======= Info =======>\n")
print(data.info())
print("\n<======= NA size =======>\n")
print(data.isna().sum())

data.groupby(['stroke']).size()

def splitClass(colm_id):
  c0,c1=[],[]
  for x in range(len(data)):
    if data['stroke'][x]==0:
      c0.append(data.iloc[x][colm_id])
    else:
      c0.append(data.iloc[x][colm_id])
  
  return c0,c1

class_1,class_2=[],[]

plt.figure(figsize=(16,20))

for x in range(len(data.columns)):
  class_0,class_1=splitClass(x)
  plt.subplot(6,2,x+1)
  plt.plot(class_0,'bo',markersize=2)
  plt.plot(class_1,'ro',markersize=5)
  plt.legend([data.columns[x]+':0',data.columns[x]+':1'])
  plt.title(data.columns[x])

plt.tight_layout()
plt.show()

# delete corrupted data and reset the index
data.dropna(inplace=True)
data.reset_index(drop=True, inplace=True)
print("\n<======= Info =======>\n")
print(data.info())
print("\n<======= NA size =======>\n")
print(data.isna().sum())

# get uniqe values in the string data 
# gender - ever_married - work_type - Residence_type - smoking_status
print("gender         :",data['gender'].unique())
print("ever_married   :",data['ever_married'].unique())
print("work_type      :",data['work_type'].unique())
print("Residence_type :",data['Residence_type'].unique())
print("smoking_status :",data['smoking_status'].unique())

# turn strings to numbers and save the labels
gender_le=LabelEncoder().fit(data['gender'])
data['gender']=gender_le.transform(data['gender'])

ever_married_le=LabelEncoder().fit(data['ever_married'])
data['ever_married']=ever_married_le.transform(data['ever_married'])

work_type_le=LabelEncoder().fit(data['work_type'])
data['work_type']=work_type_le.transform(data['work_type'])

Residence_type_le=LabelEncoder().fit(data['Residence_type'])
data['Residence_type']=Residence_type_le.transform(data['Residence_type'])

smoking_status_le=LabelEncoder().fit(data['smoking_status'])
data['smoking_status']=smoking_status_le.transform(data['smoking_status'])

data.head(10)

# normalization
normalizer=MinMaxScaler().fit(data)
data=normalizer.transform(data)

print(data)

# split the data
# just last value is output. 
input_size=10
x_data=data[:,:input_size]
y_data=data[:,input_size:]

print("\n<===== all input and output data shape =====>\n")
print("x_data : ",x_data.shape)
print("y_data : ",y_data.shape)

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.33, random_state=42)
# no need this variables enymore
del x_data,y_data,data

print("\n<===== Trian and test data shape =====>\n")
print("x_train : ",x_train.shape)
print("y_train : ",y_train.shape)
print("x_test  : ",x_test.shape)
print("y_test  : ",y_test.shape)

# create model
dropout=0.2

model=tf.keras.models.Sequential()
model.add(InputLayer(input_shape=(input_size,)))
model.add(Dropout(dropout))
model.add(Dense(10,activation='relu'))
model.add(Dropout(dropout))
model.add(Dense(10,activation='relu'))
model.add(Dropout(dropout))
model.add(Dense(1,activation='sigmoid'))

# binary(0,1) classification

model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])
model.summary()

history=model.fit(x_train,y_train,batch_size=32,epochs=25,validation_data=(x_test,y_test))

# visuelize result
plt.figure(figsize=(15,7))

plt.subplot(1,2,1)
plt.plot(history.history['loss'],'b-')
plt.plot(history.history['val_loss'],'r-')
plt.legend(['loss','val_loss'])

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'],'b-')
plt.plot(history.history['val_accuracy'],'r-')
plt.legend(['accuracy','val_accuracy'])

plt.tight_layout()
plt.show()

# evaluate model
train_eval=model.evaluate(x_train,y_train,verbose=0)
print("\n<====== Train data======>\n")
print("Loss     : ",round(train_eval[0],ndigits=3))
print("Accuracy : %",round(train_eval[1],ndigits=3)*100)

test_eval=model.evaluate(x_test,y_test,verbose=0)
print("\n<====== Test data======>\n")
print("Loss     : ",round(test_eval[0],ndigits=3))
print("Accuracy : %",round(test_eval[1],ndigits=3)*100)